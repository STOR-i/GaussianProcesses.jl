using Distributions: Normal, logpdf
using LinearAlgebra: ldiv!, diag, inv

########################
# Leave-one-out        #
########################

function predict_LOO(Î£::AbstractPDMat, alpha::AbstractVector{<:Real}, y::AbstractVector{<:Real})
    invÎ£ = inv(Î£)
    Ïƒi2 = 1 ./ diag(invÎ£)
    Î¼i = -alpha .* Ïƒi2 .+ y
    return Î¼i, Ïƒi2
end
"""
    predict_LOO(gp::GPE)

Leave-one-out cross-validated predictions. 
Returns predictions of yáµ¢ given all other observations yâ‚‹áµ¢,
as a vector of means and standard deviations.
Using the notation from Rasmussen & Williams, see e.g. equation 5.12:
    Ïƒáµ¢ = ð• (yáµ¢ | yâ‚‹áµ¢)^(1/2)
    Î¼áµ¢ = ð”¼ (yáµ¢ | yâ‚‹áµ¢)
"""
function predict_LOO(gp::GPE)
    # extract relevant bits from GPE object
    Î£ = gp.cK
    alpha = gp.alpha
    y = gp.y
    return predict_LOO(Î£, alpha, y)
end

"""
    logp_LOO(gp::GPE)

Leave-one-out log probability CV criterion.
"""
function logp_LOO(gp::GPE)
    y = gp.y
    Î¼, Ïƒ2 = predict_LOO(gp)
    return sum(logpdf(Normal(Î¼i,âˆšÏƒi2), yi) 
               for (Î¼i,Ïƒi2,yi)
               in zip(Î¼,Ïƒ2,y)
               )
end

"""
    dlogpdÎ¸_LOO_kern(gp::GPE)

Derivative of leave-one-out CV criterion with respect to the kernel hyperparameters.
See Rasmussen & Williams equations 5.13.

TODO: mean and noise parameters also.
"""
function dlogpdÎ¸_LOO_kern!(âˆ‚logpâˆ‚Î¸::AbstractVector{<:Real}, invÎ£::PDMat, kernel::Kernel, x::AbstractMatrix, y::AbstractVector, data::KernelData, alpha::AbstractVector)
    dim = num_params(kernel)
    nobs = length(y)
    @assert length(âˆ‚logpâˆ‚Î¸) == dim

    Ïƒi2 = 1 ./ diag(invÎ£)
    Î¼i = -alpha .* Ïƒi2 .+ y

    # Note: if useful, the derivatives of Î¼ and Ïƒ could be moved to a separate function.
    # âˆ‚Î¼âˆ‚Î¸ = Matrix{Float64}(undef, nobs, dim)
    # âˆ‚Ïƒâˆ‚Î¸ = Matrix{Float64}(undef, nobs, dim)
    Zj = Matrix{Float64}(undef, nobs, nobs)
    for j in 1:dim
        grad_slice!(Zj, kernel, x, data, j)
        Zj = invÎ£.mat * Zj
        # ldiv!(Î£, Zj)

        ZjÎ£inv = diag(Zj*Matrix(invÎ£))
        âˆ‚Ïƒ2âˆ‚Î¸j = ZjÎ£inv.*(Ïƒi2.^2)
        âˆ‚Î¼âˆ‚Î¸j = (Zj*alpha).*Ïƒi2 .- alpha .* âˆ‚Ïƒ2âˆ‚Î¸j
        # âˆ‚Î¼âˆ‚Î¸[:,j] = âˆ‚Î¼âˆ‚Î¸j
        # âˆ‚Ïƒâˆ‚Î¸[:,j] = âˆ‚Ïƒâˆ‚Î¸j

        âˆ‚logpâˆ‚Î¸j = 0.0
        for i in 1:nobs
            # exponentiated quadratic component:
            âˆ‚logpâˆ‚Î¸j -= 2*(y[i]-Î¼i[i]) / Ïƒi2[i] * âˆ‚Î¼âˆ‚Î¸j[i]
            âˆ‚logpâˆ‚Î¸j -= (y[i]-Î¼i[i])^2 * ZjÎ£inv[i]
            # log determinant component:
            @assert ZjÎ£inv[i] * Ïƒi2[i] â‰ˆ âˆ‚Ïƒ2âˆ‚Î¸j[i] / Ïƒi2[i]
            âˆ‚logpâˆ‚Î¸j += ZjÎ£inv[i] * Ïƒi2[i]
        end
        âˆ‚logpâˆ‚Î¸[j] = âˆ‚logpâˆ‚Î¸j
    end
    âˆ‚logpâˆ‚Î¸ .*= -1/2
    return âˆ‚logpâˆ‚Î¸
end

function dlogpdÏƒ2_LOO(invÎ£::PDMat, x::AbstractMatrix, y::AbstractVector, data::KernelData, alpha::AbstractVector)
    nobs = length(y)

    Ïƒi2 = 1 ./ diag(invÎ£)
    Î¼i = -alpha .* Ïƒi2 .+ y

    Zj = invÎ£.mat
    ZjÎ£inv = diag(Zj^2)
    âˆ‚Ïƒ2âˆ‚Ïƒ2 = ZjÎ£inv.*(Ïƒi2.^2)
    âˆ‚Î¼âˆ‚Ïƒ2 = (Zj*alpha).*Ïƒi2 .- alpha .* âˆ‚Ïƒ2âˆ‚Ïƒ2

    âˆ‚logpâˆ‚Ïƒ2 = 0.0
    for i in 1:nobs
        # exponentiated quadratic component:
        âˆ‚logpâˆ‚Ïƒ2 -= 2*(y[i]-Î¼i[i]) / Ïƒi2[i] * âˆ‚Î¼âˆ‚Ïƒ2[i]
        âˆ‚logpâˆ‚Ïƒ2 -= (y[i]-Î¼i[i])^2 * ZjÎ£inv[i]
        # log determinant component:
        @assert ZjÎ£inv[i] * Ïƒi2[i] â‰ˆ âˆ‚Ïƒ2âˆ‚Ïƒ2[i] / Ïƒi2[i]
        âˆ‚logpâˆ‚Ïƒ2 += ZjÎ£inv[i] * Ïƒi2[i]
    end
    return -âˆ‚logpâˆ‚Ïƒ2 ./ 2
end

function dlogpdÎ¸_LOO(gp::GPE; noise::Bool, domean::Bool, kern::Bool)
    Î£ = gp.cK
    x, y = gp.x, gp.y
    data = gp.data
    kernel = gp.kernel
    alpha = gp.alpha

    invÎ£ = inv(Î£)

    n_mean_params = num_params(gp.mean)
    n_kern_params = num_params(gp.kernel)
    âˆ‚logpâˆ‚Î¸ = Vector{Float64}(undef, noise + domean*n_mean_params + kern*n_kern_params)
    i = 1
    if noise
        âˆ‚logpâˆ‚Î¸[i] = dlogpdÏƒ2_LOO(invÎ£, x, y, data, alpha)*2*exp(2 * gp.logNoise)
        i += 1
    end
    if domean && n_mean_params>0
        throw("I don't know how to do means yet")
        Mgrads = grad_stack(gp.mean, gp.x)
        for j in 1:n_mean_params
            gp.dmll[i] = dot(Mgrads[:,j], gp.alpha)
            i += 1
        end
    end
    if kern
        âˆ‚logpâˆ‚Î¸_k = @view(âˆ‚logpâˆ‚Î¸[i:end])
        dlogpdÎ¸_LOO_kern!(âˆ‚logpâˆ‚Î¸_k, invÎ£, kernel, x, y, data, alpha)
    end
    return âˆ‚logpâˆ‚Î¸
end

########################
# Arbitrary fold       #
########################

const Folds = AbstractVector{<:AbstractVector{Int}}

function predict_CVfold(Î£::AbstractPDMat, alpha::AbstractVector{<:Real}, y::AbstractVector{<:Real}, folds::Folds)
    invÎ£ = inv(Î£)
    Î¼ = Vector{Float64}[]
    Î£ = Matrix{Float64}[]
    for V in folds
        Î£VT = inv(Matrix(invÎ£)[V,V])
        Î¼VT = y[V]-Î£VT*alpha[V]
        push!(Î¼, Î¼VT)
        push!(Î£, Î£VT)
    end
    return Î¼, Î£
end
"""
    predict_CVfold(gp::GPE)

Leave-one-out cross-validated predictions. 
Returns predictions of yáµ¢ given all other observations yâ‚‹áµ¢,
as a vector of means and standard deviations.
Using the notation from Rasmussen & Williams, see e.g. equation 5.12:
    Ïƒáµ¢ = ð• (yáµ¢ | yâ‚‹áµ¢)^(1/2)
    Î¼áµ¢ = ð”¼ (yáµ¢ | yâ‚‹áµ¢)
"""
function predict_CVfold(gp::GPE, folds::Folds)
    # extract relevant bits from GPE object
    Î£ = gp.cK
    alpha = gp.alpha
    y = gp.y
    return predict_CVfold(Î£, alpha, y, folds)
end

"""
    logp_CVfold(gp::GPE)

Leave-one-out log probability CV criterion.
"""
function logp_CVfold(gp::GPE, folds::Folds)
    y = gp.y
    Î¼, Î£ = predict_CVfold(gp, folds)
    CV = 0.0
    for (Î¼VT,Î£VT,V) in zip(Î¼,Î£,folds)
        chol = similar(Î£VT)
        Î£VT, chol = make_posdef!(Î£VT, chol)
        Î£PD = PDMat(Î£VT, chol)
        CV += logpdf(MvNormal(Î¼VT, Î£PD), y[V])
    end
    return CV
end

"""
    dlogpdÎ¸_CVfold_kern!(âˆ‚logpâˆ‚Î¸::AbstractVector{<:Real}, gp::GPE, folds::Folds)

Derivative of leave-one-out CV criterion with respect to the kernel hyperparameters.
See Rasmussen & Williams equations 5.13.

TODO: mean and noise parameters also.
"""
function dlogpdÎ¸_CVfold_kern!(âˆ‚logpâˆ‚Î¸::AbstractVector{<:Real}, invÎ£::PDMat, kernel::Kernel, x::AbstractMatrix, y::AbstractVector, data::KernelData, alpha::AbstractVector, folds::Folds)
    nobs = length(y)
    dim = num_params(kernel)

    @assert length(âˆ‚logpâˆ‚Î¸) == dim
    buffer1 = Matrix{Float64}(undef, nobs, nobs)
    buffer2 = Matrix{Float64}(undef, nobs, nobs)
    for j in 1:dim
        grad_slice!(buffer2, kernel, x, data, j)
        mul!(buffer1, invÎ£.mat, buffer2)
        Zj = buffer1
        # ldiv!(Î£, Zj)
        ZjÎ± = Zj*alpha

        mul!(buffer2, Zj, invÎ£.mat)
        ZjÎ£inv = buffer2

        âˆ‚logpâˆ‚Î¸j = 0.0
        for V in folds
            Î£VT = inv(invÎ£.mat[V,V])
            Î¼VT = y[V]-Î£VT*alpha[V]
            # exponentiated quadratic component:
            resid = y[V]-Î¼VT
            ZjÎ£invVV = ZjÎ£inv[V,V]
            âˆ‚logpâˆ‚Î¸j -= 2*dot(resid, ZjÎ±[V] .- ZjÎ£invVV*Î£VT*alpha[V])
            âˆ‚logpâˆ‚Î¸j -= dot(resid, ZjÎ£invVV*resid)
            # log determinant component:
            âˆ‚logpâˆ‚Î¸j += dot(ZjÎ£invVV,Î£VT)
        end
        âˆ‚logpâˆ‚Î¸[j] = âˆ‚logpâˆ‚Î¸j
    end
    âˆ‚logpâˆ‚Î¸ .*= -1/2
    return âˆ‚logpâˆ‚Î¸
end

function dlogpdÏƒ2_CVfold(invÎ£::PDMat, x::AbstractMatrix, y::AbstractVector, data::KernelData, alpha::AbstractVector, folds::Folds)
    nobs = length(y)

    Zj = invÎ£.mat
    ZjÎ± = Zj*alpha
    ZjÎ£inv = invÎ£.mat^2

    âˆ‚logpâˆ‚Ïƒ2 = 0.0
    for V in folds
        Î£VT = inv(invÎ£.mat[V,V])
        Î¼VT = y[V]-Î£VT*alpha[V]
        # exponentiated quadratic component:
        resid = y[V]-Î¼VT
        ZjÎ£invVV = ZjÎ£inv[V,V]
        âˆ‚logpâˆ‚Ïƒ2 -= 2*dot(resid, ZjÎ±[V] .- ZjÎ£invVV*Î£VT*alpha[V])
        âˆ‚logpâˆ‚Ïƒ2 -= dot(resid, ZjÎ£invVV*resid)
        # log determinant component:
        âˆ‚logpâˆ‚Ïƒ2 += dot(ZjÎ£invVV,Î£VT)
    end
    return -âˆ‚logpâˆ‚Ïƒ2 / 2
end

function dlogpdÎ¸_CVfold(gp::GPE, folds::Folds; noise::Bool, domean::Bool, kern::Bool)
    Î£ = gp.cK
    x, y = gp.x, gp.y
    data = gp.data
    kernel = gp.kernel
    alpha = gp.alpha

    invÎ£ = inv(Î£)

    n_mean_params = num_params(gp.mean)
    n_kern_params = num_params(gp.kernel)
    âˆ‚logpâˆ‚Î¸ = Vector{Float64}(undef, noise + domean*n_mean_params + kern*n_kern_params)
    i = 1
    if noise
        âˆ‚logpâˆ‚Î¸[i] = dlogpdÏƒ2_CVfold(invÎ£, x, y, data, alpha, folds)*2*exp(2 * gp.logNoise)
        i += 1
    end
    if domean && n_mean_params>0
        throw("I don't know how to do means yet")
        Mgrads = grad_stack(gp.mean, gp.x)
        for j in 1:n_mean_params
            gp.dmll[i] = dot(Mgrads[:,j], gp.alpha)
            i += 1
        end
    end
    if kern
        âˆ‚logpâˆ‚Î¸_k = @view(âˆ‚logpâˆ‚Î¸[i:end])
        dlogpdÎ¸_CVfold_kern!(âˆ‚logpâˆ‚Î¸_k, invÎ£, kernel, x, y, data, alpha, folds)
    end
    return âˆ‚logpâˆ‚Î¸
end
